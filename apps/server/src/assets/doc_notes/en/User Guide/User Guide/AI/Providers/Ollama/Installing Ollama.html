<p><a href="https://ollama.com/">Ollama</a> can be installed in a variety
  of ways, and even runs <a href="https://hub.docker.com/r/ollama/ollama">within a Docker container</a>.
  Ollama will be noticeably quicker when running on a GPU (Nvidia, AMD, Intel),
  but it can run on CPU and RAM. To install Ollama without any other prerequisites,
  you can follow their <a href="https://ollama.com/download">installer</a>:</p>
<figure
class="image image_resized" style="width:50.49%;">
  <img style="aspect-ratio:785/498;" src="3_Installing Ollama_image.png"
  width="785" height="498">
  </figure>
  <figure class="image image_resized" style="width:40.54%;">
    <img style="aspect-ratio:467/100;" src="Installing Ollama_image.png"
    width="467" height="100">
  </figure>
  <figure class="image image_resized" style="width:55.73%;">
    <img style="aspect-ratio:1296/1011;" src="1_Installing Ollama_image.png"
    width="1296" height="1011">
  </figure>
  <p>After their installer completes, if you're on Windows, you should see
    an entry in the start menu to run it:</p>
  <figure class="image image_resized"
  style="width:66.12%;">
    <img style="aspect-ratio:1161/480;" src="2_Installing Ollama_image.png"
    width="1161" height="480">
  </figure>
  <p>Also, you should have access to the <code spellcheck="false">ollama</code> CLI
    via Powershell or CMD:</p>
  <figure class="image image_resized" style="width:86.09%;">
    <img style="aspect-ratio:1730/924;" src="5_Installing Ollama_image.png"
    width="1730" height="924">
  </figure>
  <p>After Ollama is installed, you can go ahead and <code spellcheck="false">pull</code> the
    models you want to use and run. Here's a command to pull my favorite tool-compatible
    model and embedding model as of April 2025:</p><pre><code class="language-text-x-trilium-auto">ollama pull llama3.1:8b
ollama pull mxbai-embed-large</code></pre>
  <p>Also, you can make sure it's running by going to <a href="http://localhost:11434">http://localhost:11434</a> and
    you should get the following response (port 11434 being the “normal” Ollama
    port):</p>
  <figure class="image">
    <img style="aspect-ratio:585/202;" src="4_Installing Ollama_image.png"
    width="585" height="202">
  </figure>
  <p>Now that you have Ollama up and running, have a few models pulled, you're
    ready to go to go ahead and start using Ollama as both a chat provider,
    and embedding provider!</p>